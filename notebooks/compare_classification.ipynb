{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13e2ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77cb4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_df = pd.read_csv(\"../data/splits/test.csv\")\n",
    "test_df.dropna(inplace=True)\n",
    "test_texts = test_df[\"clean_text\"].tolist()\n",
    "test_labels = test_df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3bc4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for distilbert\n",
    "test_df_bert = pd.read_csv(\"../data/splits/raw/test.csv\")\n",
    "test_df_bert.dropna(inplace=True)\n",
    "test_texts_bert = test_df_bert[\"text\"].tolist()\n",
    "test_labels_bert = test_df_bert[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18afe7",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b3c2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/logistic_model.pkl\"\n",
    "\n",
    "logistic_pipeline = joblib.load(model_path)\n",
    "\n",
    "logistic_preds = logistic_pipeline.predict(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3ebf1",
   "metadata": {},
   "source": [
    "## Fine tuned distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5706ebd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"cornualghost/tm4scam-distilbert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c755a7",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4f905c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_preds = logistic_pipeline.predict(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc628e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "test_encodings = tokenizer(test_texts_bert, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_encodings = {key: val.to(device) for key, val in test_encodings.items()}\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**test_encodings)\n",
    "    logits = outputs.logits\n",
    "    bert_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tm4scam)",
   "language": "python",
   "name": "tm4scam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
